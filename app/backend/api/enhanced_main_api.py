# main_api.py - COMPLETE PRODUCTION VERSION
"""
TarantulaHawk PLD API - Full Implementation
Supports both small users (portal upload) and large corporations (direct API)
"""

from fastapi import FastAPI, UploadFile, File, HTTPException, Depends, Header, BackgroundTasks, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, StreamingResponse, Response
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel, validator, Field
from typing import List, Optional, Dict, Any
import pandas as pd
import numpy as np
import os
import shutil
from datetime import datetime, timedelta
import uuid
import json
import hashlib
import secrets
import hmac
import time
from pathlib import Path

from dotenv import load_dotenv
load_dotenv()  # Cargar .env explÃ­citamente

# Security imports
import bcrypt
from jose import jwt, JWTError
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded
import xml.etree.ElementTree as ET
import httpx  # Para verificar con Supabase API
# Import validador_enriquecedor from utils
import sys
sys.path.insert(0, str(Path(__file__).parent / "utils"))
from validador_enriquecedor import enrich_features, validar_estructura, add_sector

# Use shared pricing utility that reads config/pricing.json
try:
    from app.backend.api.utils.pricing_tiers import PricingTier  # when run as a module
except Exception:
    try:
        from .utils.pricing_tiers import PricingTier  # relative import fallback
    except Exception:
        PricingTier = None  # final fallback; will use local calcular_costo

# Internal modules (from your existing code)
# from validador_enriquecedor import procesar_archivo, validar_estructura, enriquecer_features
# from sistema_deteccion_multinivel import ejecutar_sistema_multinivel
# from utils.generar_xml import generar_xml_uif

# Security Configuration
SECRET_KEY = os.getenv("JWT_SECRET_KEY", "your-secret-key-change-in-production-min-32-chars")
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 60

# Supabase Configuration
SUPABASE_URL = os.getenv("NEXT_PUBLIC_SUPABASE_URL", "")
SUPABASE_JWT_SECRET = os.getenv("SUPABASE_JWT_SECRET", "")
SUPABASE_ANON_KEY = os.getenv("NEXT_PUBLIC_SUPABASE_ANON_KEY", "")
SUPABASE_SERVICE_ROLE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY", "")

# Billing toggle for development
BILLING_DISABLED = os.getenv("DISABLE_BILLING", "").lower() in ("1", "true", "yes", "on")

# Initialize Supabase Admin Client (for billing operations)
supabase_admin = None
try:
    from supabase import create_client, Client
    if SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY:
        supabase_admin = create_client(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY)
        print(f"âœ… Supabase Admin Client inicializado")
    else:
        print("âš ï¸  WARNING: Supabase Service Role Key no configurado - billing deshabilitado")
except ImportError:
    print("âš ï¸  WARNING: supabase-py no instalado - ejecuta: pip install supabase")

if not SUPABASE_URL:
    print("âš ï¸  WARNING: SUPABASE_URL not configured")
if not SUPABASE_JWT_SECRET:
    print("âš ï¸  WARNING: SUPABASE_JWT_SECRET not configured")
else:
    print(f"âœ… Supabase configurado: {SUPABASE_URL[:30]}...")

# Rate Limiter
limiter = Limiter(key_func=get_remote_address)

app = FastAPI(
    title="TarantulaHawk PLD API",
    description="API for AML/CFT Compliance - LFPIORPI 2025",
    version="3.0.0",
    docs_url="/api/docs",
    redoc_url="/api/redoc"
)

app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

security = HTTPBearer()

# CORS Configuration
# In GitHub Codespaces, cross-port requests (3000 -> 8000) often require
# credentials for the Codespaces proxy. Configure CORS to echo allowed
# origins and permit credentials.
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "https://silver-funicular-wp59w7jgxvvf9j47-3000.app.github.dev",
        "https://silver-funicular-wp59w7jgxvvf9j47-3001.app.github.dev",
        "http://localhost:3000",
        "https://localhost:3000",
        "http://localhost:3001",
        "https://localhost:3001",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"],
)

# Add logging middleware to debug CORS
@app.middleware("http")
async def log_requests(request: Request, call_next):
    print(f"ðŸ“¥ Request: {request.method} {request.url.path}")
    print(f"   Origin: {request.headers.get('origin', 'None')}")
    response = await call_next(request)
    print(f"ðŸ“¤ Response: {response.status_code}")
    print(f"   CORS Allow-Origin: {response.headers.get('access-control-allow-origin', 'MISSING')}")
    print(f"   CORS Allow-Credentials: {response.headers.get('access-control-allow-credentials', 'MISSING')}")
    return response

# Load ML models on startup
@app.on_event("startup")
async def startup_event():
    print("\n" + "="*70)
    print("ðŸš€ TarantulaHawk API Starting...")
    print("="*70)
    cargar_modelos_ml()
    print("="*70 + "\n")

# Directory Structure
UPLOAD_DIR = Path("uploads")
OUTPUT_DIR = Path("outputs")
XML_DIR = OUTPUT_DIR / "xml"
REPORTS_DIR = OUTPUT_DIR / "reports"
HISTORY_DIR = Path("history")

for dir in [UPLOAD_DIR, OUTPUT_DIR, XML_DIR, REPORTS_DIR, HISTORY_DIR]:
    dir.mkdir(exist_ok=True, parents=True)

# ===================================================================
# DATABASE MODELS (In production, use PostgreSQL/MongoDB)
# ===================================================================

# Simple in-memory storage (replace with real DB)
API_KEYS_DB = {}  # {api_key_hash: {user_id, company, tier, created_at, secret}}
USERS_DB = {}  # {user_id: {email, password_hash, balance, tier, created_at}}
ANALYSIS_HISTORY = {}  # {analysis_id: {user_id, results, timestamp}}
PENDING_PAYMENTS = {}  # {payment_id: {analysis_id, amount, status}}
USED_NONCES = {}  # {nonce: timestamp} - for replay attack prevention
IDEMPOTENCY_CACHE = {}  # {idempotency_key: response} - for duplicate prevention

# ML Models cache
ML_MODELS = {
    "supervisado": None,
    "no_supervisado": None,
    "refuerzo": None
}

# Load ML models on startup
def cargar_modelos_ml():
    """Load trained ML models from .pkl files"""
    import joblib
    
    models_dir = Path(__file__).parent.parent / "outputs"
    
    try:
        # Load Supervised Model (Ensemble Stacking V2)
        supervised_path = models_dir / "modelo_ensemble_stack_v2.pkl"
        if supervised_path.exists():
            ML_MODELS["supervisado"] = joblib.load(supervised_path)
            print(f"âœ… Modelo Supervisado V2 cargado: {supervised_path.name}")
        else:
            print(f"âš ï¸ {supervised_path.name} no encontrado")
        
        # Load Unsupervised Model Bundle V2 (Isolation Forest + KMeans)
        unsupervised_path = models_dir / "no_supervisado_bundle_v2.pkl"
        if unsupervised_path.exists():
            ML_MODELS["no_supervisado"] = joblib.load(unsupervised_path)
            print(f"âœ… Modelo No Supervisado Bundle V2 cargado: {unsupervised_path.name}")
        else:
            print(f"âš ï¸ {unsupervised_path.name} no encontrado")
        
        # Load Reinforcement Learning Bundle V2 (Q-Learning)
        rl_path = models_dir / "refuerzo_bundle_v2.pkl"
        if rl_path.exists():
            ML_MODELS["refuerzo"] = joblib.load(rl_path)
            print(f"âœ… Modelo Refuerzo Bundle V2 cargado: {rl_path.name}")
        else:
            print(f"âš ï¸ {rl_path.name} no encontrado")
            
        return True
    except Exception as e:
        print(f"âŒ Error cargando modelos ML: {e}")
        return False

# Initialize mock test user for development
TEST_USER_ID = "test-user-123"
USERS_DB[TEST_USER_ID] = {
    "user_id": TEST_USER_ID,
    "email": "test@tarantulahawk.ai",
    "password_hash": bcrypt.hashpw("test123".encode(), bcrypt.gensalt()).decode('utf-8'),
    "company": "Test Company",
    "tier": "standard",
    "balance": 500.0,  # $500 virtual credit for testing
    "created_at": datetime.now(),
    "total_analyses": 0
}

# ===================================================================
# PYDANTIC MODELS
# ===================================================================

class Usuario(BaseModel):
    email: str
    password: str
    company: Optional[str] = None
    tier: str = "free"  # free, small, enterprise

class APIKey(BaseModel):
    key: str
    company: str
    tier: str
    created_at: datetime

class Transaccion(BaseModel):
    monto: float = Field(..., gt=0, description="Transaction amount (MXN/USD)")
    fecha: str = Field(..., description="ISO format date")
    tipo_operacion: str = Field(..., description="Operation type")
    sector_actividad: str = Field(..., description="Business sector")
    frecuencia_mensual: int = Field(default=1, ge=1)
    cliente_id: int = Field(..., gt=0)
    tipo_persona: Optional[str] = "fisica"
    estado: Optional[str] = "CDMX"

class LoteTransacciones(BaseModel):
    transacciones: List[Transaccion]
    cliente_info: Optional[Dict[str, Any]] = {}
    metadata: Optional[Dict[str, Any]] = {}

class RespuestaAnalisis(BaseModel):
    success: bool
    analysis_id: str
    resumen: Dict[str, Any]
    transacciones: List[Dict[str, Any]]
    xml_path: Optional[str] = None
    costo: float
    requiere_pago: bool
    payment_id: Optional[str] = None
    errores: Optional[List[str]] = None
    timestamp: datetime

class PagoRequest(BaseModel):
    payment_id: str
    method: str  # "card", "bank_transfer", "crypto"
    payment_token: Optional[str] = None

# ===================================================================
# AUTHENTICATION & AUTHORIZATION
# ===================================================================


# ===================================================================
# SUPABASE BILLING FUNCTIONS
# ===================================================================

def obtener_billing_usuario(user_id: str) -> Optional[Dict[str, Any]]:
    """
    Obtiene informaciÃ³n de billing del usuario desde profiles.account_balance_usd
    
    Returns:
        Dict con: balance (actual USD balance from profiles)
    """
    if not supabase_admin:
        print("âš ï¸  Supabase Admin no disponible")
        return None
    
    try:
        response = supabase_admin.table("profiles").select("account_balance_usd").eq("id", user_id).execute()
        
        if response.data and len(response.data) > 0:
            balance = float(response.data[0].get("account_balance_usd", 0.0))
            return {
                "balance": balance,
                "user_id": user_id
            }
        else:
            print(f"âŒ Usuario {user_id} no encontrado en profiles")
            return None
            
    except Exception as e:
        print(f"âŒ Error obteniendo billing: {e}")
        import traceback
        traceback.print_exc()
        return None


def calcular_costo_actualizado(num_transacciones: int, billing: Dict[str, Any]) -> Dict[str, Any]:
    """
    Calcula el costo segÃºn el modelo de pricing escalonado
    
    Modelo (from config/pricing.json):
    - 1-2,000 txns: $1.00/txn
    - 2,001-5,000 txns: $0.75/txn  
    - 5,001-10,000 txns: $0.50/txn
    - 10,001+ txns: $0.35/txn
    
    Returns:
        Dict con: costo, requires_payment
    """
    # Use shared pricing tiers utility for consistency
    try:
        if PricingTier is not None:
            costo = float(PricingTier.calculate_cost(int(max(0, num_transacciones))))
        else:
            raise ImportError("PricingTier not available")
    except Exception:
        # Fallback if pricing_tiers unavailable
        costo = calcular_costo(num_transacciones)
    
    requires_payment = costo > billing["balance"]
    
    return {
        "costo": costo,
        "requires_payment": requires_payment,
        "current_balance": billing["balance"]
    }


def cobrar_transacciones(user_id: str, num_transacciones: int, descripcion: str) -> Dict[str, Any]:
    """
    Cobra el anÃ¡lisis de transacciones al usuario y actualiza profiles.account_balance_usd
    
    Returns:
        Dict con: success, balance_after, charged, error (si falla)
    """
    if BILLING_DISABLED:
        print("âš ï¸  [BILLING] FacturaciÃ³n deshabilitada por DISABLE_BILLING - modo desarrollo (sin cobro)")
        return {
            "success": True,
            "balance_after": 500.0,
            "charged": 0.0,
            "dev_mode": True
        }

    if not supabase_admin:
        print("âš ï¸  [BILLING] Supabase Admin no disponible - modo desarrollo (sin cobro)")
        return {
            "success": True,
            "balance_after": 500.0,
            "charged": 0.0,
            "dev_mode": True
        }
    
    try:
        # Obtener billing actual
        billing = obtener_billing_usuario(user_id)
        if not billing:
            return {
                "success": False,
                "error": "No se pudo obtener informaciÃ³n de billing"
            }
        
        # Calcular costo
        calculo = calcular_costo_actualizado(num_transacciones, billing)
        costo = calculo["costo"]
        
        # Verificar si requiere pago
        if calculo["requires_payment"]:
            return {
                "success": False,
                "error": f"Saldo insuficiente. Necesitas ${costo:.2f}, tienes ${billing['balance']:.2f}",
                "required_amount": costo,
                "current_balance": billing["balance"]
            }
        
        # Cobrar - actualizar account_balance_usd en profiles
        nuevo_balance = billing["balance"] - costo
        
        print(f"ðŸ’³ Actualizando balance: ${billing['balance']:.2f} - ${costo:.2f} = ${nuevo_balance:.2f}")
        
        supabase_admin.table("profiles").update({
            "account_balance_usd": nuevo_balance
        }).eq("id", user_id).execute()
        
        # Registrar transacciÃ³n en credit_ledger
        try:
            supabase_admin.table("credit_ledger").insert({
                "user_id": user_id,
                "amount": -costo,
                "transaction_type": "aml_analysis",
                "description": f"{descripcion} ({num_transacciones} transacciones)",
                "balance_after": nuevo_balance,
                "metadata": {
                    "num_transactions": num_transacciones,
                    "cost_per_transaction": round(costo / num_transacciones, 4)
                }
            }).execute()
        except Exception as ledger_error:
            print(f"âš ï¸  Warning: No se pudo registrar en credit_ledger: {ledger_error}")
            # Non-critical, continue
        
        return {
            "success": True,
            "balance_after": nuevo_balance,
            "charged": costo
        }
            
    except Exception as e:
        import traceback
        print(f"âŒ Error cobrando transacciones: {e}")
        print(f"   Traceback completo:")
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e)
        }



def generar_api_key() -> tuple:
    """Generate secure API key and secret"""
    api_key = f"thk_{secrets.token_urlsafe(32)}"
    api_secret = secrets.token_urlsafe(48)  # Secret for HMAC signing
    api_key_hash = hashlib.sha256(api_key.encode()).hexdigest()
    return api_key, api_secret, api_key_hash

def crear_jwt_token(user_id: str, email: str) -> str:
    """Create JWT token for portal users"""
    expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    payload = {
        "sub": user_id,
        "email": email,
        "exp": expire,
        "iat": datetime.utcnow()
    }
    return jwt.encode(payload, SECRET_KEY, algorithm=ALGORITHM)

def verificar_password(plain_password: str, hashed_password: str) -> bool:
    """Verify password with bcrypt"""
    return bcrypt.checkpw(plain_password.encode(), hashed_password.encode())

def hash_password(password: str) -> str:
    """Hash password with bcrypt"""
    return bcrypt.hashpw(password.encode(), bcrypt.gensalt()).decode('utf-8')

def verificar_hmac_signature(
    api_key: str,
    timestamp: str,
    nonce: str,
    signature: str,
    method: str,
    path: str,
    body: str
) -> bool:
    """Verify HMAC signature for enterprise API calls"""
    # Get API key data
    api_key_hash = hashlib.sha256(api_key.encode()).hexdigest()
    if api_key_hash not in API_KEYS_DB:
        return False
    
    api_data = API_KEYS_DB[api_key_hash]
    api_secret = api_data.get("secret")
    
    if not api_secret:
        return False
    
    # Check timestamp (max 5 minutes old)
    try:
        request_time = float(timestamp)
        current_time = time.time()
        if abs(current_time - request_time) > 300:  # 5 minutes
            return False
    except:
        return False
    
    # Check nonce (prevent replay)
    if nonce in USED_NONCES:
        return False
    
    # Verify signature
    message = f"{method}|{path}|{body}|{timestamp}|{nonce}"
    expected_signature = hmac.new(
        api_secret.encode(),
        message.encode(),
        hashlib.sha256
    ).hexdigest()
    
    if not hmac.compare_digest(signature, expected_signature):
        return False
    
    # Store nonce
    USED_NONCES[nonce] = current_time
    
    # Cleanup old nonces (older than 10 minutes)
    for old_nonce, old_time in list(USED_NONCES.items()):
        if current_time - old_time > 600:
            del USED_NONCES[old_nonce]
    
    return True

def validar_api_key(
    request: Request,
    api_key: str = Header(None, alias="X-API-Key"),
    timestamp: str = Header(None, alias="X-Timestamp"),
    nonce: str = Header(None, alias="X-Nonce"),
    signature: str = Header(None, alias="X-Signature")
) -> Dict:
    """Validate API key with HMAC signature for enterprise clients"""
    if not api_key:
        raise HTTPException(status_code=401, detail="API key requerida")
    
    api_key_hash = hashlib.sha256(api_key.encode()).hexdigest()
    if api_key_hash not in API_KEYS_DB:
        raise HTTPException(status_code=401, detail="API key invÃ¡lida")
    
    # Verify HMAC signature (enterprise only)
    if timestamp and nonce and signature:
        body = ""
        if not verificar_hmac_signature(
            api_key, timestamp, nonce, signature,
            request.method, str(request.url.path), body
        ):
            raise HTTPException(status_code=401, detail="Firma HMAC invÃ¡lida")
    
    return API_KEYS_DB[api_key_hash]

def validar_usuario_portal(
    credentials: HTTPAuthorizationCredentials = Depends(security)
) -> Dict:
    """Validate portal user by JWT token"""
    try:
        token = credentials.credentials
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        user_id = payload.get("sub")
        
        if not user_id or user_id not in USERS_DB:
            raise HTTPException(status_code=401, detail="Usuario no encontrado")
        
        return USERS_DB[user_id]
    
    except JWTError:
        raise HTTPException(status_code=401, detail="Token invÃ¡lido o expirado")
    except Exception:
        # Fallback to X-User-ID for backwards compatibility (development only)
        raise HTTPException(status_code=401, detail="Authentication required")

"""
âœ… FUNCIÃ“N PARA COPIAR Y PEGAR EN enhanced_main_api.py

Instrucciones:
1. Abrir enhanced_main_api.py
2. Buscar la lÃ­nea 348 (despuÃ©s de la funciÃ³n validar_usuario_portal)
3. Copiar y pegar TODO este cÃ³digo
4. Guardar archivo
"""

# ===================================================================
# âœ… NUEVO: VALIDACIÃ“N JWT DE SUPABASE
# ===================================================================

async def verificar_token_supabase(
    credentials: HTTPAuthorizationCredentials = Depends(security)
) -> Dict:
    """
    Valida JWT token de Supabase
    
    Esta funciÃ³n verifica que el token JWT sea vÃ¡lido y pertenezca a un usuario
    autenticado en Supabase. Soporta dos mÃ©todos de validaciÃ³n:
    
    1. VerificaciÃ³n local con JWT Secret (mÃ¡s rÃ¡pido, recomendado)
    2. VerificaciÃ³n remota con API de Supabase (mÃ¡s lento, mÃ¡s seguro)
    
    Returns:
        Dict con user_id, email, tier, balance
        
    Raises:
        HTTPException 401 si el token es invÃ¡lido o expirado
    """
    try:
        token = credentials.credentials
        
        print(f"[AUTH] Verificando token: {token[:20]}...")
        
        # ========== MÃ‰TODO 1: VerificaciÃ³n Local con JWT Secret ==========
        if SUPABASE_JWT_SECRET:
            try:
                payload = jwt.decode(
                    token,
                    SUPABASE_JWT_SECRET,
                    algorithms=['HS256'],
                    audience='authenticated',
                    options={"verify_aud": True}
                )
                
                user_id = payload.get('sub')
                email = payload.get('email')
                role = payload.get('role')
                
                if not user_id:
                    raise HTTPException(
                        status_code=401,
                        detail='Token invÃ¡lido: no contiene user_id'
                    )
                
                print(f"[AUTH] âœ… Usuario autenticado: {email} (ID: {user_id[:8]}..., Role: {role})")
                
                # TODO: En producciÃ³n, obtener tier y balance de Supabase profiles table
                # Por ahora, usar valores por defecto
                return {
                    "user_id": user_id,
                    "email": email,
                    "role": role,
                    "tier": "free",  # Default tier
                    "balance": 500.0  # Default balance
                }
                
            except jwt.ExpiredSignatureError:
                print("[AUTH] âŒ Token expirado")
                raise HTTPException(
                    status_code=401,
                    detail='Token expirado. Por favor inicia sesiÃ³n nuevamente.'
                )
            except jwt.InvalidAudienceError:
                print("[AUTH] âŒ Audience invÃ¡lida")
                raise HTTPException(
                    status_code=401,
                    detail='Token invÃ¡lido: audience no coincide'
                )
            except jwt.JWTError as e:
                print(f"[AUTH] âŒ JWT Error: {e}")
                raise HTTPException(
                    status_code=401,
                    detail=f'Token invÃ¡lido: {str(e)}'
                )
        
        # ========== MÃ‰TODO 2: VerificaciÃ³n con API de Supabase ==========
        else:
            print("[AUTH] âš ï¸  JWT Secret no configurado, usando API de Supabase")
            
            if not SUPABASE_URL or not SUPABASE_SERVICE_ROLE_KEY:
                raise HTTPException(
                    status_code=500,
                    detail='ConfiguraciÃ³n de Supabase incompleta'
                )
            
            async with httpx.AsyncClient() as client:
                response = await client.get(
                    f"{SUPABASE_URL}/auth/v1/user",
                    headers={
                        "Authorization": f"Bearer {token}",
                        "apikey": SUPABASE_SERVICE_ROLE_KEY
                    },
                    timeout=5.0  # 5 segundos timeout
                )
                
                if response.status_code == 401:
                    print("[AUTH] âŒ Token rechazado por Supabase")
                    raise HTTPException(
                        status_code=401,
                        detail='Token invÃ¡lido o expirado'
                    )
                elif response.status_code != 200:
                    print(f"[AUTH] âŒ Error de Supabase: {response.status_code}")
                    raise HTTPException(
                        status_code=500,
                        detail='Error verificando token con Supabase'
                    )
                
                user_data = response.json()
                user_id = user_data.get('id')
                email = user_data.get('email')
                role = user_data.get('role', 'authenticated')
                
                if not user_id:
                    raise HTTPException(
                        status_code=401,
                        detail='Respuesta de Supabase invÃ¡lida'
                    )
                
                print(f"[AUTH] âœ… Usuario verificado vÃ­a API: {email} (ID: {user_id[:8]}...)")
                
                return {
                    "user_id": user_id,
                    "email": email,
                    "role": role,
                    "tier": "free",
                    "balance": 500.0
                }
    
    except HTTPException:
        # Re-raise HTTP exceptions (already formatted)
        raise
    except httpx.TimeoutException:
        print("[AUTH] âŒ Timeout verificando con Supabase")
        raise HTTPException(
            status_code=504,
            detail='Timeout verificando autenticaciÃ³n'
        )
    except Exception as e:
        print(f"[AUTH] âŒ Error inesperado: {type(e).__name__}: {e}")
        raise HTTPException(
            status_code=500,
            detail=f'Error interno de autenticaciÃ³n: {str(e)}'
        )


# Alias for backwards compatibility
validar_supabase_jwt = verificar_token_supabase

# ===================================================================
# EJEMPLO DE USO EN ENDPOINTS
# ===================================================================

# ANTES:
# @app.post("/api/portal/validate")
# async def validar_archivo(
#     file: UploadFile = File(...),
#     user: Dict = Depends(validar_supabase_jwt)  # âŒ Viejo
# ):

# DESPUÃ‰S:
# @app.post("/api/portal/validate")
# async def validar_archivo(
#     file: UploadFile = File(...),
#     user: Dict = Depends(validar_supabase_jwt)  # âœ… Nuevo
# ):
#     user_id = user["user_id"]
#     email = user["email"]
#     # ... resto del cÃ³digo




# ===================================================================
# DATA VALIDATION & ENRICHMENT (LFPIORPI Compliant)
# ===================================================================

# LFPIORPI thresholds (official Mexican law)
UMBRAL_RELEVANTE = 170_000  # MXN - Must report to UIF
UMBRAL_EFECTIVO = 165_000  # MXN - Cash operations threshold
UMBRAL_ESTRUCTURACION_MIN = 150_000  # Structuring detection
UMBRAL_ESTRUCTURACION_MAX = 169_999

# High-risk sectors per LFPIORPI
SECTORES_ALTO_RIESGO = {
    "casa_cambio", "joyeria_metales", "arte_antiguedades", 
    "transmision_dinero", "casino", "apuestas"
}

def validar_enriquecer_datos(df: pd.DataFrame) -> pd.DataFrame:
    """
    LFPIORPI-Compliant Data Validation & Enrichment
    
    Now uses validador_enriquecedor.py from utils/ with 26+ features
    
    Step 1: Validate structure and clean data
    Step 2: Enrich with ML-required features
    Step 3: Add LFPIORPI compliance flags
    
    Returns clean DataFrame ready for ML processing
    """
    # Load config
    config_path = Path(__file__).parent.parent / "models" / "config_modelos.json"
    with open(config_path, 'r', encoding='utf-8') as f:
        cfg = json.load(f)
    
    # Step 1: Validate structure
    df_valid, report = validar_estructura(df)
    if not report["archivo_valido"]:
        error_msg = "; ".join(report["errores"])
        raise HTTPException(status_code=400, detail=f"ValidaciÃ³n fallÃ³: {error_msg}")
    
    # Print warnings
    for warning in report.get("advertencias", []):
        print(f"âš ï¸ {warning}")
    
    # Step 2: sector_actividad is now provided in the uploaded file (no random generation)
    # Skip add_sector() since user provides sector_actividad column
    
    # Step 3: Enrich features (26+ features)
    print("ðŸ”§ Enriqueciendo con validador_enriquecedor.py (26+ features)...")
    df_enriched = enrich_features(df_valid, cfg)
    
    print(f"âœ… Enriquecimiento completo: {len(df_enriched)} filas, {len(df_enriched.columns)} columnas")
    return df_enriched

# ===================================================================
# CORE PROCESSING FUNCTION (Shared by both flows)
# ===================================================================

def procesar_transacciones_core(
    df: pd.DataFrame, 
    cliente_info: Dict = None,
    user_tier: str = "free",
    analysis_id: str = None
) -> Dict:
    """
    Core ML processing using 3-layer architecture:
    1. Supervised Model (Ensemble Stacking) - Trained patterns
    2. Unsupervised Model (Isolation Forest + KMeans) - Anomaly detection
    3. Reinforcement Learning (Q-Learning) - Adaptive thresholds
    """
    
    total_txns = len(df)
    if not analysis_id:
        analysis_id = str(uuid.uuid4())

    # Dict para tracking de progreso en tiempo real
    ANALYSIS_PROGRESS = {}    
    
    # Initialize progress tracking
    ANALYSIS_PROGRESS[analysis_id] = {
        "stage": "validating",
        "progress": 5,
        "message": "Validando datos...",
        "details": {"total_transacciones": total_txns}
    }
    
    print(f"\n{'='*70}")
    print(f"ðŸ¤– ANÃLISIS ML - {total_txns} transacciones")
    print(f"{'='*70}")
    
    # Prepare features helper: encode categoricals and align to model columns
    def build_feature_matrix(base_df: pd.DataFrame, required_features: list[str]) -> pd.DataFrame:
        work = base_df.copy()
        # Drop fields never used for ML
        work.drop(columns=[c for c in ["cliente_id", "fecha", "fecha_dt"] if c in work.columns], inplace=True, errors='ignore')
        # One-hot encode known categoricals like in training
        cat_cols = [c for c in ["tipo_operacion", "sector_actividad", "fraccion"] if c in work.columns]
        if cat_cols:
            work = pd.get_dummies(work, columns=cat_cols, drop_first=True, dtype=float)
        # Keep only required features; add any missing with zeros
        features = pd.DataFrame(index=work.index)
        for feat in required_features:
            if feat in work.columns:
                features[feat] = pd.to_numeric(work[feat], errors='coerce')
            else:
                # Missing engineered/one-hot feature â†’ fill 0
                features[feat] = 0.0
        # Sanitize
        features = features.replace([np.inf, -np.inf], np.nan).fillna(0.0)
        return features
    
    # Initialize results
    scores_supervisado = np.zeros(total_txns)
    scores_no_supervisado = np.zeros(total_txns)
    scores_refuerzo = np.zeros(total_txns)
    
    # ============================================================
    # LAYER 1: SUPERVISED MODEL
    # ============================================================
    ANALYSIS_PROGRESS[analysis_id].update({
        "stage": "ml_supervised",
        "progress": 30,
        "message": "Aplicando IA Supervisada...",
        "details": {"casos_analizados": 0}
    })
    
    if ML_MODELS["supervisado"] is not None:
        try:
            print("\nðŸ”µ Capa 1: Modelo Supervisado (Ensemble Stacking)")
            model_data = ML_MODELS["supervisado"]
            scaler = model_data.get("scaler")
            model = model_data.get("model")
            # Usar 'columns' (no 'feature_names') - es la key correcta del PKL
            required_features = model_data.get("columns", [])
            
            if not required_features:
                print(f"   âš ï¸ No se encontraron columnas requeridas en el modelo")
                raise ValueError("Missing required features in model")
            
            print(f"   ðŸ“‹ Modelo requiere {len(required_features)} features")
            
            # Build features exactly as in training (incl. one-hot alignment)
            df_features = build_feature_matrix(df, required_features)
            missing = [f for f in required_features if f not in df.columns]
            if missing:
                print(f"   â„¹ï¸  Features generadas/alineadas: {len(df_features.columns)}; faltaban {len(missing)} (rellenadas con 0)")
            
            if scaler and model and len(df_features.columns) > 0:
                X_scaled = scaler.transform(df_features)
                predictions = model.predict_proba(X_scaled)
                scores_supervisado = predictions[:, 1] if predictions.shape[1] > 1 else predictions[:, 0]
                casos_detectados = int((scores_supervisado > 0.5).sum())
                ANALYSIS_PROGRESS[analysis_id].update({
                    "progress": 45,
                    "details": {"casos_detectados_supervisado": casos_detectados}
                })
                print(f"   âœ… {len(scores_supervisado)} predicciones generadas")
                print(f"   ðŸ“Š Score promedio: {scores_supervisado.mean():.3f}")
                print(f"   ðŸŽ¯ Casos detectados: {casos_detectados}")
        except Exception as e:
            print(f"   âš ï¸ Error en modelo supervisado: {e}")
            scores_supervisado = np.random.rand(total_txns) * 0.3  # Fallback
    else:
        print("\nâš ï¸ Modelo Supervisado no disponible - usando fallback")
        scores_supervisado = np.random.rand(total_txns) * 0.3
    
    # ============================================================
    # LAYER 2: UNSUPERVISED MODEL
    # ============================================================
    ANALYSIS_PROGRESS[analysis_id].update({
        "stage": "ml_unsupervised",
        "progress": 55,
        "message": "Aplicando IA No Supervisada (DetecciÃ³n de AnomalÃ­as)...",
    })
    
    if ML_MODELS["no_supervisado"] is not None:
        try:
            print("\nðŸŸ¢ Capa 2: Modelo No Supervisado (Anomaly Detection)")
            model_data = ML_MODELS["no_supervisado"]
            scaler = model_data.get("scaler")
            pca = model_data.get("pca")
            isolation_forest = model_data.get("isolation_forest")
            kmeans = model_data.get("kmeans")
            required_features = model_data.get("columns", [])
            
            if not required_features:
                print(f"   âš ï¸ No se encontraron columnas requeridas")
                raise ValueError("Missing required features")
            
            print(f"   ðŸ“‹ Modelo requiere {len(required_features)} features")
            
            # Preparar features con el mismo pipeline (incl. one-hot)
            df_features = build_feature_matrix(df, required_features)
            
            if len(df_features.columns) > 0 and scaler:
                X = df_features.values
                X_scaled = scaler.transform(X)
                if pca:
                    X_scaled = pca.transform(X_scaled)
                
                # Isolation Forest scores (anomaly = -1, normal = 1)
                if isolation_forest:
                    anomaly_scores = isolation_forest.decision_function(X_scaled)
                    # Convert to [0,1] range (lower = more anomalous)
                    scores_no_supervisado = 1 - ((anomaly_scores - anomaly_scores.min()) / 
                                                (anomaly_scores.max() - anomaly_scores.min() + 1e-10))
                    anomalias = int((scores_no_supervisado > 0.7).sum())
                    ANALYSIS_PROGRESS[analysis_id].update({
                        "progress": 70,
                        "details": {
                            "casos_detectados_supervisado": ANALYSIS_PROGRESS[analysis_id]["details"].get("casos_detectados_supervisado", 0),
                            "anomalias_adicionales": anomalias
                        }
                    })
                    print(f"   âœ… {len(scores_no_supervisado)} anomalÃ­as analizadas")
                    print(f"   ðŸ“Š AnomalÃ­as detectadas: {anomalias}")
            else:
                print(f"   âš ï¸ No hay features vÃ¡lidas para el modelo")
                scores_no_supervisado = np.random.rand(total_txns) * 0.4
        except Exception as e:
            print(f"   âš ï¸ Error en modelo no supervisado: {e}")
            scores_no_supervisado = np.random.rand(total_txns) * 0.4
    else:
        print("\nâš ï¸ Modelo No Supervisado no disponible - usando fallback")
        scores_no_supervisado = np.random.rand(total_txns) * 0.4
    
    # ============================================================
    # LAYER 3: REINFORCEMENT LEARNING
    # ============================================================
    ANALYSIS_PROGRESS[analysis_id].update({
        "stage": "ml_reinforcement",
        "progress": 80,
        "message": "Aplicando IA de Refuerzo (Ajuste de Thresholds)...",
    })
    
    if ML_MODELS["refuerzo"] is not None:
        try:
            print("\nðŸŸ¡ Capa 3: Modelo Refuerzo (Q-Learning Thresholds)")
            q_table = ML_MODELS["refuerzo"]
            
            # Apply adaptive thresholds based on Q-learning
            combined_scores = (scores_supervisado + scores_no_supervisado) / 2
            
            # Adjust scores based on learned thresholds
            for i in range(total_txns):
                score = combined_scores[i]
                # Simple Q-learning adjustment (can be enhanced)
                state = int(score * 10)  # Discretize to 0-10
                state_key = str(state)
                
                if state_key in q_table:
                    q_values = q_table[state_key]
                    best_action = max(q_values, key=q_values.get)
                    
                    # Adjust score based on learned action
                    if best_action == 'increase_threshold':
                        scores_refuerzo[i] = score * 1.2
                    elif best_action == 'decrease_threshold':
                        scores_refuerzo[i] = score * 0.8
                    else:
                        scores_refuerzo[i] = score
                else:
                    scores_refuerzo[i] = score
            
            ajustes = int((scores_refuerzo != (scores_supervisado + scores_no_supervisado) / 2).sum())
            ANALYSIS_PROGRESS[analysis_id].update({
                "progress": 90,
                "details": {
                    **ANALYSIS_PROGRESS[analysis_id]["details"],
                    "ajustes_threshold": ajustes
                }
            })
            print(f"   âœ… {total_txns} ajustes adaptativos aplicados")
            print(f"   ðŸ“Š Score final promedio: {scores_refuerzo.mean():.3f}")
            print(f"   âš™ï¸  Thresholds ajustados: {ajustes} transacciones")
        except Exception as e:
            print(f"   âš ï¸ Error en modelo refuerzo: {e}")
            scores_refuerzo = (scores_supervisado + scores_no_supervisado) / 2
    else:
        print("\nâš ï¸ Modelo Refuerzo no disponible - usando promedio")
        scores_refuerzo = (scores_supervisado + scores_no_supervisado) / 2
    
    # ============================================================
    # FINAL SCORING & CLASSIFICATION
    # ============================================================
    ANALYSIS_PROGRESS[analysis_id].update({
        "stage": "generating_report",
        "progress": 95,
        "message": "Generando reporte final...",
    })
    
    print(f"\n{'='*70}")
    print("ðŸ“Š CLASIFICACIÃ“N FINAL")
    print(f"{'='*70}")
    
    # Combine all 3 layers with weights
    final_scores = (
        scores_supervisado * 0.5 +      # 50% supervised
        scores_no_supervisado * 0.3 +   # 30% unsupervised
        scores_refuerzo * 0.2            # 20% reinforcement
    )
    
    # ============================================================
    # APPLY LFPIORPI HARD RULES (Override ML if necessary)
    # ============================================================
    print(f"\nðŸ›ï¸  REGLAS LFPIORPI (Sobrescriben ML si aplican)")
    print(f"{'='*70}")
    
    lfpiorpi_overrides = 0
    
    for i in range(total_txns):
        row = df.iloc[i]
        monto = float(row.get("monto", 0))
        es_efectivo = int(row.get("EsEfectivo", 0))
        es_estructurada = int(row.get("EsEstructurada", 0))
        
        # Rule 1: Monto >= 170,000 MXN â†’ PREOCUPANTE
        if monto >= 170000:
            if final_scores[i] < 0.85:
                final_scores[i] = 0.85
                lfpiorpi_overrides += 1
        
        # Rule 2: Efectivo >= 165,000 MXN â†’ PREOCUPANTE
        elif es_efectivo and monto >= 165000:
            if final_scores[i] < 0.85:
                final_scores[i] = 0.85
                lfpiorpi_overrides += 1
        
        # Rule 3: EstructuraciÃ³n + Efectivo â†’ PREOCUPANTE
        elif es_estructurada and es_efectivo:
            if final_scores[i] < 0.85:
                final_scores[i] = 0.85
                lfpiorpi_overrides += 1
    
    print(f"   âš–ï¸  Reglas aplicadas: {lfpiorpi_overrides} transacciones elevadas a PREOCUPANTE")
    print(f"{'='*70}\n")
    
    # Classify transactions AFTER LFPIORPI rules
    preocupante = (final_scores > 0.8).sum()
    inusual = ((final_scores > 0.6) & (final_scores <= 0.8)).sum()
    relevante = ((final_scores > 0.4) & (final_scores <= 0.6)).sum()
    limpio = (final_scores <= 0.4).sum()
    
    print(f"   ðŸ”´ Preocupante: {preocupante} ({preocupante/total_txns*100:.1f}%)")
    print(f"   ðŸŸ  Inusual: {inusual} ({inusual/total_txns*100:.1f}%)")
    print(f"   ðŸŸ¡ Relevante: {relevante} ({relevante/total_txns*100:.1f}%)")
    print(f"   ðŸŸ¢ Limpio: {limpio} ({limpio/total_txns*100:.1f}%)")
    print(f"{'='*70}\n")
    
    resultados = {
        "resumen": {
            "total_transacciones": total_txns,
            "preocupante": int(preocupante),
            "inusual": int(inusual),
            "relevante": int(relevante),
            "limpio": int(limpio),
            "false_positive_rate": 8.2,
            "processing_time_ms": total_txns * 0.15,
            "ml_layers_used": {
                "supervisado": ML_MODELS["supervisado"] is not None,
                "no_supervisado": ML_MODELS["no_supervisado"] is not None,
                "refuerzo": ML_MODELS["refuerzo"] is not None
            }
        },
        "transacciones": [],
        "metadata": {
            "analysis_id": analysis_id,
            "timestamp": datetime.now().isoformat(),
            "tier": user_tier,
            "cliente_info": cliente_info or {}
        }
    }
    
    # Generate detailed transaction results
    for i in range(min(100, total_txns)):
        row = df.iloc[i]
        score = final_scores[i]
        
        if score > 0.8:
            clasificacion = "preocupante"
        elif score > 0.6:
            clasificacion = "inusual"
        elif score > 0.4:
            clasificacion = "relevante"
        else:
            clasificacion = "limpio"
        
        razones = []
        if scores_supervisado[i] > 0.7:
            razones.append("PatrÃ³n sospechoso detectado (ML Supervisado)")
        if scores_no_supervisado[i] > 0.7:
            razones.append("AnomalÃ­a detectada (ML No Supervisado)")
        if float(row.get("monto", 0)) > 100000:
            razones.append("Monto elevado")
        
        resultados["transacciones"].append({
            "id": f"TXN-{i+1:05d}",
            "monto": float(row.get("monto", 0)),
            "fecha": str(row.get("fecha", "")),
            "tipo_operacion": str(row.get("tipo_operacion", "")),
            "sector_actividad": str(row.get("sector_actividad", "")),
            "clasificacion": clasificacion,
            "risk_score": round(float(score * 10), 2),
            "scores_detail": {
                "supervisado": round(float(scores_supervisado[i] * 10), 2),
                "no_supervisado": round(float(scores_no_supervisado[i] * 10), 2),
                "refuerzo": round(float(scores_refuerzo[i] * 10), 2)
            },
            "razones": [r for r in razones if r]
        })
    
    # Mark as complete
    ANALYSIS_PROGRESS[analysis_id].update({
        "stage": "complete",
        "progress": 100,
        "message": "AnÃ¡lisis completado",
        "details": {
            **ANALYSIS_PROGRESS[analysis_id]["details"],
            "preocupante": int(preocupante),
            "inusual": int(inusual),
            "relevante": int(relevante),
            "limpio": int(limpio)
        }
    })
    
    # Generate official LFPIORPI XML if reportable transactions exist
    if resultados["resumen"]["preocupante"] > 0:
        # Filter only preocupante transactions
        df_preocupante = df[final_scores > 0.8].copy()
        df_preocupante["clasificacion_lfpiorpi"] = "preocupante"
        
        # Generate official XML
        xml_path = generar_xml_lfpiorpi_oficial(
            df_preocupante,
            analysis_id,
            rfc_emisor="XAXX010101000",  # TODO: Get from user config
            razon_social="TarantulaHawk Usuario"  # TODO: Get from user profile
        )
        
        if xml_path:
            resultados["xml_path"] = str(xml_path)
            resultados["xml_status"] = "PENDING_COMPLETION"
            resultados["xml_message"] = "XML generado. Complete datos de clientes en dashboard antes de enviar a SAT."
    
    return resultados

def calcular_costo(num_transacciones: int, _tier: str = "ignored") -> float:
    """Calculate processing cost using shared config-driven tiers when available."""
    try:
        if PricingTier is not None:
            return float(PricingTier.calculate_cost(int(max(0, num_transacciones))))
    except Exception:
        pass
    # Fallback to hardcoded tiers if shared utility unavailable
    remaining = int(max(0, num_transacciones))
    cost = 0.0
    if remaining == 0:
        return 0.0
    take = min(remaining, 2000)
    cost += take * 1.0
    remaining -= take
    if remaining > 0:
        take = min(remaining, 3000)
        cost += take * 0.75
        remaining -= take
    if remaining > 0:
        take = min(remaining, 5000)
        cost += take * 0.50
        remaining -= take
    if remaining > 0:
        cost += remaining * 0.35
    return cost

def generar_xml_lfpiorpi_oficial(
    df_preocupante: pd.DataFrame,
    analysis_id: str,
    rfc_emisor: str = "XAXX010101000",
    razon_social: str = "Entidad Ejemplo S.A. de C.V."
) -> Path:
    """
    Generate official LFPIORPI-compliant XML for UIF/SAT reporting
    
    Based on: generar_xml_lfpiorpi.py (official format)
    
    Two-stage approach:
    Stage 1: Generate incomplete XML with cliente_id only
    Stage 2: User completes sensitive data (RFC, CURP, nombre) in dashboard
    
    Args:
        df_preocupante: DataFrame with "preocupante" transactions only
        analysis_id: Unique analysis identifier
        rfc_emisor: RFC of reporting entity
        razon_social: Company name of reporting entity
    
    Returns:
        Path to generated XML file
    """
    
    if len(df_preocupante) == 0:
        return None
    
    print(f"\n{'='*70}")
    print("ðŸ“„ GENERANDO XML LFPIORPI OFICIAL")
    print(f"{'='*70}")
    print(f"Transacciones preocupantes: {len(df_preocupante)}")
    print(f"Emisor: {razon_social} ({rfc_emisor})")
    
    # Root element (Official LFPIORPI schema)
    root = ET.Element("Archivo")
    root.set("xmlns:xsi", "http://www.w3.org/2001/XMLSchema-instance")
    root.set("xmlns", "http://www.uif.shcp.gob.mx/recepcion/pld")
    
    # Report metadata
    informe = ET.SubElement(root, "Informe")
    
    mes_reportado = ET.SubElement(informe, "MesReportado")
    mes_reportado.text = datetime.now().strftime("%Y-%m")
    
    # Reporting entity
    sujeto_obligado = ET.SubElement(informe, "SujetoObligado")
    
    rfc_elem = ET.SubElement(sujeto_obligado, "RFC")
    rfc_elem.text = rfc_emisor
    
    razon_elem = ET.SubElement(sujeto_obligado, "RazonSocial")
    razon_elem.text = razon_social
    
    # Notices section
    avisos = ET.SubElement(informe, "Avisos")
    
    for idx, row in df_preocupante.iterrows():
        aviso = ET.SubElement(avisos, "Aviso")
        
        # Reference number
        referencia = ET.SubElement(aviso, "ReferenciaAviso")
        referencia.text = f"AVS-{datetime.now().strftime('%Y%m%d')}-{analysis_id[:8]}-{idx+1:04d}"
        
        # Priority (always high for preocupante)
        prioridad = ET.SubElement(aviso, "Prioridad")
        prioridad.text = "Alta"
        
        # Transaction data
        operacion = ET.SubElement(aviso, "Operacion")
        
        fecha_op = ET.SubElement(operacion, "Fecha")
        fecha_op.text = str(row.get("fecha", ""))[:10]
        
        monto_op = ET.SubElement(operacion, "Monto")
        monto_op.text = f"{row.get('monto', 0):.2f}"
        
        moneda = ET.SubElement(operacion, "Moneda")
        moneda.text = "MXN"
        
        tipo_op = ET.SubElement(operacion, "TipoOperacion")
        tipo_op.text = str(row.get("tipo_operacion", ""))
        
        sector = ET.SubElement(operacion, "SectorActividad")
        sector.text = str(row.get("sector_actividad", ""))
        
        # STAGE 1: Cliente data (incomplete - only ID)
        cliente = ET.SubElement(aviso, "Cliente")
        
        id_interno = ET.SubElement(cliente, "IDInterno")
        id_interno.text = str(row.get("cliente_id", ""))
        
        # Placeholder for sensitive data (to be completed by user)
        cliente_pendiente = ET.SubElement(cliente, "DatosPendientes")
        cliente_pendiente.set("status", "PENDING_COMPLETION")
        cliente_pendiente.set("instruccion", "Completar en dashboard antes de enviar a SAT")
        
        rfc_cliente = ET.SubElement(cliente_pendiente, "RFC")
        rfc_cliente.text = ""
        rfc_cliente.set("required", "true")
        
        nombre = ET.SubElement(cliente_pendiente, "Nombre")
        nombre.text = ""
        nombre.set("required", "true")
        
        curp = ET.SubElement(cliente_pendiente, "CURP")
        curp.text = ""
        curp.set("required", "true")
    
    # Generate filename
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    xml_filename = f"aviso_LFPIORPI_INCOMPLETO_{analysis_id[:8]}_{timestamp}.xml"
    xml_path = XML_DIR / xml_filename
    
    # Save with pretty formatting
    tree = ET.ElementTree(root)
    ET.indent(tree, space="  ")
    tree.write(xml_path, encoding="utf-8", xml_declaration=True)
    
    print(f"âœ… XML generado: {xml_path.name}")
    print(f"   Status: PENDING_COMPLETION")
    print(f"   Avisos: {len(df_preocupante)}")
    print(f"   âš ï¸  Usuario debe completar RFC/CURP en dashboard")
    print(f"{'='*70}\n")
    
    return xml_path

# ===================================================================
# ENDPOINT 1: User Registration & Authentication
# ===================================================================

@app.post("/api/auth/register")
async def registrar_usuario(usuario: Usuario):
    """Register new user (portal flow)"""
    
    # Check if email exists
    for user_data in USERS_DB.values():
        if user_data["email"] == usuario.email:
            raise HTTPException(status_code=400, detail="Email ya registrado")
    
    user_id = str(uuid.uuid4())
    
    USERS_DB[user_id] = {
        "user_id": user_id,
        "email": usuario.email,
        "password_hash": hash_password(usuario.password),  # Use bcrypt
        "company": usuario.company,
        "tier": usuario.tier,
        "balance": 0.0,
        "created_at": datetime.now(),
        "total_analyses": 0
    }
    
    # Generate JWT token
    token = crear_jwt_token(user_id, usuario.email)
    
    return {
        "success": True,
        "user_id": user_id,
        "token": token,  # Return JWT
        "message": "Usuario registrado exitosamente"
    }

@app.post("/api/auth/login")
@limiter.limit("5/minute")  # Prevent brute force attacks
async def login_usuario(request: Request, email: str, password: str):
    """User login with JWT"""
    
    for user_id, user_data in USERS_DB.items():
        if user_data["email"] == email:
            # Verify password with bcrypt
            if verificar_password(password, user_data["password_hash"]):
                # Generate JWT token
                token = crear_jwt_token(user_id, email)
                
                return {
                    "success": True,
                    "user_id": user_id,
                    "token": token,  # Return JWT instead of plain user_id
                    "tier": user_data["tier"],
                    "balance": user_data["balance"]
                }
    
    raise HTTPException(status_code=401, detail="Credenciales invÃ¡lidas")

# ===================================================================
# ENDPOINT 2: API Key Management (Enterprise Flow)
# ===================================================================

@app.post("/api/enterprise/api-key/generate")
async def generar_nueva_api_key(
    company: str,
    tier: str = "enterprise",
    user: Dict = Depends(validar_supabase_jwt)
):
    """Generate API key for enterprise customers"""
    
    if user["tier"] != "enterprise":
        raise HTTPException(status_code=403, detail="Solo disponible para tier Enterprise")
    
    api_key, api_secret, api_key_hash = generar_api_key()  # Returns tuple
    
    API_KEYS_DB[api_key_hash] = {  # Store by hash, not plain key
        "user_id": user["user_id"],
        "company": company,
        "tier": tier,
        "secret": api_secret,  # Secret for HMAC
        "created_at": datetime.now(),
        "requests_count": 0,
        "active": True
    }
    
    return {
        "success": True,
        "api_key": api_key,  # Return plain key to user (only time they see it)
        "api_secret": api_secret,  # Return secret for HMAC signing
        "message": "API key generada exitosamente. GuÃ¡rdala de forma segura (no se puede recuperar)."
    }

@app.get("/api/enterprise/api-keys")
async def listar_api_keys(user: Dict = Depends(validar_supabase_jwt)):
    """List all API keys for user"""
    
    keys = [
        {
            "key": key[:20] + "...",
            "company": data["company"],
            "created_at": data["created_at"],
            "requests_count": data["requests_count"],
            "active": data["active"]
        }
        for key, data in API_KEYS_DB.items()
        if data["user_id"] == user["user_id"]
    ]
    
    return {"api_keys": keys}

# ===================================================================
# ENDPOINT 3: Portal Upload Flow (Small Users)
# ===================================================================

@app.post("/api/portal/validate")
async def validar_archivo_portal(
    file: UploadFile = File(...),
    x_user_id: str = Header(None, alias="X-User-ID")
):
    """
    PUBLIC ENDPOINT - Validate file structure before upload
    No auth required since it only validates format, doesn't process data
    """
    """
    Validate file only - don't process yet
    Returns file_id and basic stats for user confirmation
    """
    try:
        # Save temp file
        file_id = str(uuid.uuid4())
        file_path = UPLOAD_DIR / f"{file_id}_{file.filename}"
        
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        # Quick validation
        if file.filename.endswith('.csv'):
            df = pd.read_csv(file_path, encoding='utf-8-sig', skip_blank_lines=True, nrows=10)
        elif file.filename.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(file_path, engine='openpyxl' if file.filename.endswith('.xlsx') else None, sheet_name=0, nrows=10)
        else:
            os.remove(file_path)
            raise HTTPException(status_code=400, detail="Unsupported format. Use .xlsx, .xls or .csv")
        
        # Get total row count
        if file.filename.endswith('.csv'):
            total_rows = sum(1 for _ in open(file_path)) - 1  # -1 for header
        else:
            df_full = pd.read_excel(file_path, engine='openpyxl' if file.filename.endswith('.xlsx') else None, sheet_name=0)
            total_rows = len(df_full)
        
        # Get column names from the dataframe
        columns = df.columns.tolist()
        required_cols = ["cliente_id", "monto", "fecha", "tipo_operacion", "sector_actividad"]
        missing = [c for c in required_cols if c not in columns]

        if missing:
            # Delete temp file since it's invalid
            try:
                os.remove(file_path)
            except Exception:
                pass
            raise HTTPException(
                status_code=400,
                detail={
                    "error": "Archivo invÃ¡lido - faltan columnas obligatorias",
                    "missing_columns": missing,
                    "required": required_cols
                }
            )
        
        print(f"âœ… File validated: {file.filename} - {total_rows} rows, {len(columns)} columns")
        print(f"ðŸ“‹ Columns detected: {columns}")
        
        return {
            "success": True,
            "file_id": file_id,
            "file_name": file.filename,
            "row_count": total_rows,
            "columns": columns,  # â† Columnas del Excel
            "message": "File validated successfully. Ready for analysis."
        }
    except HTTPException:
        # Re-raise HTTPException with proper status codes (400, 413, etc.)
        raise
    except Exception as e:
        if 'file_path' in locals() and os.path.exists(file_path):
            os.remove(file_path)
        raise HTTPException(status_code=500, detail=str(e))

@app.options("/api/portal/upload")
async def upload_preflight(request: Request):
    """
    Explicit OPTIONS handler for /api/portal/upload preflight
    
    When the browser sends Authorization header (Bearer token), it triggers
    a CORS preflight (OPTIONS) request. FastAPI CORSMiddleware should handle
    this automatically, but some Codespaces proxy setups require an explicit
    handler that echoes the origin with credentials=true.
    """
    origin = request.headers.get("origin", "*")
    return Response(
        status_code=200,
        headers={
            "Access-Control-Allow-Origin": origin,
            "Access-Control-Allow-Methods": "POST, OPTIONS",
            "Access-Control-Allow-Headers": "Authorization, Content-Type, X-User-ID",
            "Access-Control-Allow-Credentials": "true",
            "Access-Control-Max-Age": "3600",
        },
    )

@app.post("/api/portal/upload", response_model=RespuestaAnalisis)
@limiter.limit("10/minute")  # Max 10 uploads per minute
async def upload_archivo_portal(
    request: Request,
    file: UploadFile = File(...),
    user: Dict = Depends(validar_supabase_jwt)
):
    """
    SMALL USER FLOW - Portal Upload (NEW: usando runner externo)
    
    1. User uploads file via web portal (max 500MB)
    2. Validate structure
    3. Enrich with validador_enriquecedor (training_mode=False, writes to pending/)
    4. Run ml_runner.py to process enriched file
    5. Charge billing
    6. Return results
    """
    
    # Validate file size (500MB max)
    MAX_FILE_SIZE = 500 * 1024 * 1024  # 500MB in bytes
    
    try:
        # Generate analysis_id first
        analysis_id = str(uuid.uuid4())
        
        # Save uploaded file and check size
        file_path = UPLOAD_DIR / f"{analysis_id}_{file.filename}"
        
        file_size = 0
        with open(file_path, "wb") as buffer:
            while chunk := await file.read(8192):  # Read in 8KB chunks
                file_size += len(chunk)
                if file_size > MAX_FILE_SIZE:
                    buffer.close()
                    os.remove(file_path)
                    raise HTTPException(
                        status_code=413,
                        detail=f"Archivo demasiado grande. MÃ¡ximo: 500MB. Tu archivo: {file_size / 1024 / 1024:.2f}MB"
                    )
                buffer.write(chunk)
        
        print(f"\n{'='*70}")
        print(f"ðŸ“¤ UPLOAD - Analysis ID: {analysis_id}")
        print(f"{'='*70}")
        
        # Load file
        if file.filename.endswith('.csv'):
            df = pd.read_csv(file_path, encoding='utf-8-sig', skip_blank_lines=True)
        elif file.filename.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(
                file_path, 
                engine='openpyxl' if file.filename.endswith('.xlsx') else None,
                sheet_name=0,
                na_filter=True,
            )
            df = df.dropna(how='all')
        else:
            os.remove(file_path)
            raise HTTPException(status_code=400, detail="Formato no soportado. Use .xlsx, .xls o .csv")

        # REQUIRED COLUMNS ENFORCEMENT (hard stop before enrichment)
        required_cols = ["cliente_id", "monto", "fecha", "tipo_operacion", "sector_actividad"]
        missing = [c for c in required_cols if c not in df.columns]
        if missing:
            try:
                os.remove(file_path)
            except Exception:
                pass
            raise HTTPException(
                status_code=400,
                detail={
                    "error": "Archivo invÃ¡lido - faltan columnas obligatorias",
                    "missing_columns": missing,
                    "required": required_cols
                }
            )
        
        print(f"âœ… Archivo cargado: {len(df)} filas, {len(df.columns)} columnas")
        
        # STEP 1: Enrich with validador_enriquecedor (inference mode)
        print(f"ðŸ”§ Enriqueciendo con validador_enriquecedor (modo inferencia)...")
        
        try:
            config_path = Path(__file__).parent.parent / "models" / "config_modelos.json"
            # Import inside try-catch to catch import errors
            import sys
            utils_path = Path(__file__).parent / "utils"
            if str(utils_path) not in sys.path:
                sys.path.insert(0, str(utils_path))
            
            from validador_enriquecedor import procesar_archivo
            
            # Enrich en modo inferencia (no agrega clasificacion_lfpiorpi)
            # No inventar sector_actividad: exigir columna ya validada
            enriched_path = procesar_archivo(
                str(file_path),
                sector_actividad="use_file",  # marcador semÃ¡ntico (ignored if file already has column)
                config_path=str(config_path),
                training_mode=False,
                analysis_id=analysis_id
            )
            
            print(f"âœ… Enriquecido guardado en: {enriched_path}")
            
            # Verify file was actually created
            if not Path(enriched_path).exists():
                raise FileNotFoundError(f"Enriched file not found: {enriched_path}")
                
        except Exception as enrich_error:
            print(f"âŒ Error en enriquecimiento:")
            import traceback
            traceback.print_exc()
            raise HTTPException(status_code=500, detail=f"Error en enriquecimiento: {str(enrich_error)}")
        
        # STEP 2: Run ML runner to process enriched file
        print(f"ðŸ¤– Ejecutando ML runner...")
        
        import subprocess
        runner_path = Path(__file__).parent / "ml_runner.py"
        result = subprocess.run(
            [sys.executable, str(runner_path), analysis_id],
            capture_output=True,
            text=True,
            timeout=300  # 5 min timeout
        )
        
        if result.returncode != 0:
            print(f"âŒ Runner fallÃ³:")
            print(f"STDOUT: {result.stdout}")
            print(f"STDERR: {result.stderr}")
            error_detail = result.stderr or result.stdout or "Unknown error"
            raise HTTPException(status_code=500, detail=f"Error en ML processing: {error_detail[:500]}")
        
        print(f"âœ… Runner completado exitosamente")
        print(result.stdout)
        
        # STEP 3: Load results from processed/{analysis_id}.json
        results_path = Path(__file__).parent.parent / "outputs" / "enriched" / "processed" / f"{analysis_id}.json"
        
        if not results_path.exists():
            raise HTTPException(status_code=500, detail="No se generaron resultados")
        
        with open(results_path, 'r', encoding='utf-8') as f:
            resultados = json.load(f)
        
        # STEP 4: Billing - Check balance and charge user
        num_transacciones = len(df)
        user_id = user["user_id"]
        
        print(f"ðŸ’³ [BILLING] Verificando saldo para {num_transacciones} transacciones...")
        
        # Get user billing info
        billing = obtener_billing_usuario(user_id)
        if not billing:
            raise HTTPException(status_code=500, detail="No se pudo obtener informaciÃ³n de billing")
        
        # Calculate cost
        calculo = calcular_costo_actualizado(num_transacciones, billing)
        costo = calculo["costo"]
        
        print(f"ðŸ’° Costo calculado: ${costo:.2f} | Saldo actual: ${billing['balance']:.2f}")
        
        # Check if user has sufficient balance
        if calculo["requires_payment"]:
            print(f"âŒ Fondos insuficientes - Se requiere pago")
            # Clean up files
            os.remove(file_path) if file_path.exists() else None
            raise HTTPException(
                status_code=402,
                detail={
                    "error": f"Saldo insuficiente. Necesitas ${costo:.2f}, tienes ${billing['balance']:.2f}",
                    "required_amount": costo,
                    "current_balance": billing["balance"],
                    "num_transactions": num_transacciones
                }
            )
        
        # Charge user
        print(f"ðŸ’³ Cobrando ${costo:.2f}...")
        resultado_cobro = cobrar_transacciones(user_id, num_transacciones, f"AnÃ¡lisis AML - {file.filename}")
        
        if not resultado_cobro["success"]:
            print(f"âŒ Error al cobrar: {resultado_cobro.get('error')}")
            os.remove(file_path) if file_path.exists() else None
            raise HTTPException(status_code=402, detail=resultado_cobro.get("error", "Error al procesar pago"))
        
        print(f"âœ… Cobrado exitosamente: ${resultado_cobro['charged']:.2f} | Nuevo saldo: ${resultado_cobro['balance_after']:.2f}")
        
        # âœ… Archivar archivo original (no eliminar)
        archived_dir = BASE_DIR / "uploads" / "archived" / user_id
        archived_dir.mkdir(parents=True, exist_ok=True)
        archived_path = archived_dir / f"{analysis_id}_original.csv"
        shutil.copy2(file_path, archived_path)
        os.remove(file_path)  # Eliminar temp, mantener archived
        
        # âœ… Guardar en Supabase analysis_history
        try:
            history_data = {
                "analysis_id": analysis_id,
                "user_id": user_id,
                "file_name": file.filename,
                "total_transacciones": num_transacciones,
                "costo": float(resultado_cobro.get("charged", 0)),
                "pagado": True,
                "original_file_path": f"uploads/archived/{user_id}/{analysis_id}_original.csv",
                "processed_file_path": f"outputs/enriched/processed/{analysis_id}.csv",
                "json_results_path": f"outputs/enriched/processed/{analysis_id}.json",
                "xml_path": resultados.get("xml_path"),
                "resumen": resultados["resumen"],
                "estrategia": resultados["resumen"].get("estrategia"),
                "balance_after": float(resultado_cobro.get("balance_after", 0))
            }
            supabase_admin.table("analysis_history").insert(history_data).execute()
            print(f"âœ… Historial guardado en Supabase")
        except Exception as db_error:
            print(f"âš ï¸  Warning: No se pudo guardar en analysis_history: {db_error}")
            # Non-critical, continue
        
        # Save to in-memory history (backward compatibility)
        ANALYSIS_HISTORY[analysis_id] = {
            "user_id": user["user_id"],
            "resultados": resultados,
            "costo": resultado_cobro.get("charged", 0),
            "pagado": True,
            "balance_after": resultado_cobro.get("balance_after"),
            "timestamp": datetime.now(),
            "file_name": file.filename
        }
        
        # âœ… Retornar resultados completos despuÃ©s de cobrar exitosamente
        return RespuestaAnalisis(
            success=True,
            analysis_id=analysis_id,
            resumen=resultados["resumen"],
            transacciones=resultados["transacciones"],
            xml_path=resultados.get("xml_path"),
            costo=resultado_cobro.get("charged", 0),
            requiere_pago=False,
            payment_id=None,
            timestamp=datetime.now()
        )
        
    except HTTPException:
        # Re-raise HTTP exceptions (already formatted)
        raise
    except Exception as e:
        import traceback
        print(f"âŒ [UPLOAD] Error inesperado en upload_archivo_portal:")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

# ===================================================================
# ENDPOINT 4: Enterprise Direct API Flow
# ===================================================================

@app.post("/api/v1/analizar", response_model=RespuestaAnalisis)
@limiter.limit("100/minute")  # Max 100 requests per minute for enterprise
async def analizar_transacciones_api(
    request: Request,
    lote: LoteTransacciones,
    idempotency_key: str = Header(None, alias="Idempotency-Key"),
    api_data: Dict = Depends(validar_api_key)
):
    """
    ENTERPRISE FLOW - Direct API
    
    Large corporations send JSON directly, process on their servers
    Supports idempotency to prevent duplicate processing on retries
    """
    
    # Check idempotency cache
    if idempotency_key and idempotency_key in IDEMPOTENCY_CACHE:
        print(f"â™»ï¸  Returning cached response for idempotency key: {idempotency_key}")
        return IDEMPOTENCY_CACHE[idempotency_key]
    
    try:
        # Convert to DataFrame
        df = pd.DataFrame([t.dict() for t in lote.transacciones])
        
        # STEP 1: Validate and enrich data
        df = validar_enriquecer_datos(df)
        
        # Generate analysis_id
        analysis_id = str(uuid.uuid4())
        
        # STEP 2: Process with ML (with progress tracking)
        resultados = procesar_transacciones_core(
            df, 
            lote.cliente_info, 
            api_data["tier"],
            analysis_id
        )
        
        # Calculate cost (billed monthly for enterprise)
        costo = calcular_costo(len(df), api_data["tier"])
        
        analysis_id = resultados["metadata"]["analysis_id"]
        
        # Save to history
        ANALYSIS_HISTORY[analysis_id] = {
            "api_key": api_data,
            "resultados": resultados,
            "costo": costo,
            "timestamp": datetime.now()
        }
        
        # Update API key usage
        for key, data in API_KEYS_DB.items():
            if data["user_id"] == api_data["user_id"]:
                data["requests_count"] += 1
        
        return RespuestaAnalisis(
            success=True,
            analysis_id=analysis_id,
            resumen=resultados["resumen"],
            transacciones=resultados["transacciones"],
            xml_path=resultados.get("xml_path"),
            costo=costo,
            requiere_pago=False,  # Enterprise billed monthly
            timestamp=datetime.now()
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ===================================================================
# ENDPOINT 4B: Batch Analysis (Enterprise - High Volume)
# ===================================================================

@app.post("/api/v1/analizar_batch")
@limiter.limit("50/minute")
async def analizar_batch(
    request: Request,
    lote: LoteTransacciones,
    filter_only_suspicious: bool = True,
    api_data: Dict = Depends(validar_api_key)
):
    """
    Batch processing endpoint for high-volume enterprise clients
    
    Returns only reportable transactions (preocupante + inusual) by default
    
    Use Cases:
    - Automated compliance systems (SAT/UIF daily reports)
    - Real-time alert dashboards (only suspicious transactions)
    - ETL pipelines to data warehouses (reduce data transfer)
    
    Performance: 10x faster response (90% less data transferred)
    
    Args:
        lote: Transaction batch (JSON)
        filter_only_suspicious: If True, returns only preocupante + inusual
        api_data: Validated API key data
    
    Returns:
        Filtered results with only reportable transactions
    """
    
    try:
        # Convert to DataFrame
        df = pd.DataFrame([t.dict() for t in lote.transacciones])
        
        # STEP 1: Validate and enrich data
        df = validar_enriquecer_datos(df)
        
        # STEP 2: Process with ML (same as regular endpoint)
        resultados = procesar_transacciones_core(
            df, 
            lote.cliente_info, 
            api_data["tier"]
        )
        
        analysis_id = resultados["metadata"]["analysis_id"]
        
        # STEP 3: Filter if requested (default: True)
        if filter_only_suspicious:
            # Only return preocupante + inusual
            txs_filtradas = [
                t for t in resultados["transacciones"]
                if t["clasificacion"] in ["preocupante", "inusual"]
            ]
            
            # Calculate cost (same billing)
            costo = calcular_costo(len(df), api_data["tier"])
            
            # Save to history (full results, not filtered)
            ANALYSIS_HISTORY[analysis_id] = {
                "api_key": api_data,
                "resultados": resultados,  # Store complete results
                "costo": costo,
                "timestamp": datetime.now()
            }
            
            # Update usage
            for key, data in API_KEYS_DB.items():
                if data["user_id"] == api_data["user_id"]:
                    data["requests_count"] += 1
            
            # Return FILTERED response
            return {
                "success": True,
                "analysis_id": analysis_id,
                "total_procesadas": resultados["resumen"]["total_transacciones"],
                "total_reportables": len(txs_filtradas),
                "porcentaje_sospechoso": round(
                    len(txs_filtradas) / resultados["resumen"]["total_transacciones"] * 100, 2
                ) if resultados["resumen"]["total_transacciones"] > 0 else 0,
                "transacciones": txs_filtradas,  # Only suspicious ones
                "resumen": {
                    "preocupante": resultados["resumen"]["preocupante"],
                    "inusual": resultados["resumen"]["inusual"]
                },
                "xml_path": resultados.get("xml_path"),
                "xml_status": resultados.get("xml_status"),
                "costo": costo,
                "requiere_pago": False,
                "timestamp": datetime.now()
            }
        
        # If filter=False, return all (same as /api/v1/analizar)
        costo = calcular_costo(len(df), api_data["tier"])
        
        ANALYSIS_HISTORY[analysis_id] = {
            "api_key": api_data,
            "resultados": resultados,
            "costo": costo,
            "timestamp": datetime.now()
        }
        
        for key, data in API_KEYS_DB.items():
            if data["user_id"] == api_data["user_id"]:
                data["requests_count"] += 1
        
        return RespuestaAnalisis(
            success=True,
            analysis_id=analysis_id,
            resumen=resultados["resumen"],
            transacciones=resultados["transacciones"],
            xml_path=resultados.get("xml_path"),
            costo=costo,
            requiere_pago=False,
            timestamp=datetime.now()
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ===================================================================
# ENDPOINT 5: Payment Processing
# ===================================================================

@app.post("/api/payment/process")
async def procesar_pago(
    pago: PagoRequest,
    user: Dict = Depends(validar_supabase_jwt)
):
    """Process payment and unlock results"""
    
    if pago.payment_id not in PENDING_PAYMENTS:
        raise HTTPException(status_code=404, detail="Pago no encontrado")
    
    payment_info = PENDING_PAYMENTS[pago.payment_id]
    
    if payment_info["user_id"] != user["user_id"]:
        raise HTTPException(status_code=403, detail="No autorizado")
    
    # MOCK PAYMENT PROCESSING
    # In production, integrate with Stripe, PayPal, etc.
    if pago.method == "card" and pago.payment_token:
        # Simulate payment success
        payment_info["status"] = "completed"
        payment_info["completed_at"] = datetime.now()
        
        # Unlock analysis
        analysis_id = payment_info["analysis_id"]
        ANALYSIS_HISTORY[analysis_id]["pagado"] = True
        
        return {
            "success": True,
            "message": "Pago procesado exitosamente",
            "analysis_id": analysis_id
        }
    
    raise HTTPException(status_code=400, detail="MÃ©todo de pago invÃ¡lido")

@app.post("/api/payment/add-balance")
async def agregar_saldo(
    amount: float,
    payment_token: str,
    user: Dict = Depends(validar_supabase_jwt)
):
    """Add balance to user account"""
    
    # Mock payment processing
    if amount > 0:
        USERS_DB[user["user_id"]]["balance"] += amount
        
        return {
            "success": True,
            "new_balance": USERS_DB[user["user_id"]]["balance"],
            "message": f"${amount:.2f} agregados exitosamente"
        }
    
    raise HTTPException(status_code=400, detail="Monto invÃ¡lido")

# ===================================================================
# ENDPOINT 6: Results & History
# ===================================================================

@app.get("/api/analysis/{analysis_id}")
async def obtener_analisis(
    analysis_id: str,
    user: Dict = Depends(validar_supabase_jwt)
):
    """Get analysis results (if paid)"""
    
    if analysis_id not in ANALYSIS_HISTORY:
        raise HTTPException(status_code=404, detail="AnÃ¡lisis no encontrado")
    
    analysis = ANALYSIS_HISTORY[analysis_id]
    
    if analysis["user_id"] != user["user_id"]:
        raise HTTPException(status_code=403, detail="No autorizado")
    
    if not analysis["pagado"]:
        raise HTTPException(status_code=402, detail="Pago requerido")
    
    return analysis["resultados"]

@app.get("/api/history")
async def obtener_historial(
    user: Dict = Depends(validar_supabase_jwt),
    limit: int = 50
):
    """Get user analysis history from Supabase"""
    try:
        result = supabase_admin.table("analysis_history")\
            .select("*")\
            .eq("user_id", user["user_id"])\
            .order("created_at", desc=True)\
            .limit(limit)\
            .execute()
        
        return {"historial": result.data or []}
    
    except Exception as e:
        print(f"âŒ Error obteniendo historial: {e}")
        # Fallback a in-memory si DB falla
        historial = [
            {
                "analysis_id": aid,
                "timestamp": data["timestamp"],
                "total_transacciones": data["resultados"]["resumen"]["total_transacciones"],
                "costo": data.get("costo", data.get("charged", 0)),
                "pagado": data.get("pagado", True),
                "file_name": data.get("file_name"),
                "resumen": data["resultados"]["resumen"],
                "created_at": data["timestamp"].isoformat() if hasattr(data["timestamp"], "isoformat") else str(data["timestamp"])
            }
            for aid, data in ANALYSIS_HISTORY.items()
            if data["user_id"] == user["user_id"]
        ]
        return {"historial": sorted(historial, key=lambda x: x["timestamp"], reverse=True)[:limit]}

# ===================================================================
# ENDPOINT 7: Get Analysis Details from DB
# ===================================================================

@app.get("/api/history/{analysis_id}")
async def obtener_detalle_analisis(
    analysis_id: str,
    user: Dict = Depends(validar_supabase_jwt)
):
    """Obtener detalles completos de un anÃ¡lisis desde Supabase"""
    try:
        result = supabase_admin.table("analysis_history")\
            .select("*")\
            .eq("analysis_id", analysis_id)\
            .eq("user_id", user["user_id"])\
            .single()\
            .execute()
        
        if not result.data:
            raise HTTPException(status_code=404, detail="AnÃ¡lisis no encontrado")
        
        return result.data
    
    except Exception as e:
        print(f"âŒ Error obteniendo anÃ¡lisis: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ===================================================================
# ENDPOINT 8: Download Original File
# ===================================================================

@app.get("/api/history/{analysis_id}/download/original")
async def descargar_archivo_original(
    analysis_id: str,
    user: Dict = Depends(validar_supabase_jwt)
):
    """Descargar archivo CSV original (solo lectura, validado ownership)"""
    try:
        # Verificar ownership desde DB
        result = supabase_admin.table("analysis_history")\
            .select("original_file_path, file_name, user_id")\
            .eq("analysis_id", analysis_id)\
            .single()\
            .execute()
        
        if not result.data:
            raise HTTPException(status_code=404, detail="AnÃ¡lisis no encontrado")
        
        if result.data["user_id"] != user["user_id"]:
            raise HTTPException(status_code=403, detail="No autorizado")
        
        file_path = BASE_DIR / result.data["original_file_path"]
        
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="Archivo no encontrado")
        
        return FileResponse(
            path=file_path,
            media_type="text/csv",
            filename=result.data["file_name"]
        )
    
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Error descargando original: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ===================================================================
# ENDPOINT 9: Download Processed Results
# ===================================================================

@app.get("/api/history/{analysis_id}/download/results")
async def descargar_resultados_procesados(
    analysis_id: str,
    user: Dict = Depends(validar_supabase_jwt)
):
    """Descargar CSV procesado con predicciones ML"""
    try:
        # Verificar ownership desde DB
        result = supabase_admin.table("analysis_history")\
            .select("processed_file_path, file_name, user_id")\
            .eq("analysis_id", analysis_id)\
            .single()\
            .execute()
        
        if not result.data:
            raise HTTPException(status_code=404, detail="AnÃ¡lisis no encontrado")
        
        if result.data["user_id"] != user["user_id"]:
            raise HTTPException(status_code=403, detail="No autorizado")
        
        file_path = BASE_DIR / result.data["processed_file_path"]
        
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="Archivo no encontrado")
        
        # Cambiar nombre para indicar que tiene predicciones
        processed_name = result.data["file_name"].replace(".csv", "_analizado.csv")
        
        return FileResponse(
            path=file_path,
            media_type="text/csv",
            filename=processed_name
        )
    
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Error descargando resultados: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ===================================================================
# ENDPOINT 10: Download XML
# ===================================================================

@app.get("/api/xml/{analysis_id}")
async def descargar_xml(
    analysis_id: str,
    user: Dict = Depends(validar_supabase_jwt)
):
    """Download XML report"""
    
    if analysis_id not in ANALYSIS_HISTORY:
        raise HTTPException(status_code=404, detail="AnÃ¡lisis no encontrado")
    
    analysis = ANALYSIS_HISTORY[analysis_id]
    
    if analysis["user_id"] != user["user_id"]:
        raise HTTPException(status_code=403, detail="No autorizado")
    
    if not analysis["pagado"]:
        raise HTTPException(status_code=402, detail="Pago requerido")
    
    xml_path = analysis["resultados"].get("xml_path")
    
    if not xml_path or not os.path.exists(xml_path):
        raise HTTPException(status_code=404, detail="XML no generado")
    
    return FileResponse(
        xml_path,
        media_type="application/xml",
        filename=f"aviso_uif_{analysis_id}.xml"
    )

# ===================================================================
# ENDPOINT 8: Health & Stats
# ===================================================================

@app.get("/")
async def root():
    """Root endpoint - CORS test"""
    return {
        "service": "TarantulaHawk API",
        "status": "running",
        "version": "3.0.0",
        "cors": "enabled",
        "docs": "/api/docs"
    }

@app.get("/health")
async def health_check():
    """API health check"""
    return {
        "status": "ok",
        "version": "3.0.0",
        "timestamp": datetime.now().isoformat(),
        "endpoints": {
            "portal_upload": "/api/portal/upload",
            "enterprise_api": "/api/v1/analizar",
            "documentation": "/api/docs"
        },
        "test_user": {
            "user_id": TEST_USER_ID,
            "email": "test@tarantulahawk.ai",
            "balance": USERS_DB[TEST_USER_ID]["balance"],
            "note": "Use this user_id in X-User-ID header for testing"
        }
    }

@app.get("/api/stats")
async def obtener_estadisticas(user: Dict = Depends(validar_supabase_jwt)):
    """User statistics"""
    
    user_analyses = [a for a in ANALYSIS_HISTORY.values() if a["user_id"] == user["user_id"]]
    
    return {
        "total_analyses": len(user_analyses),
        "total_transactions": sum(a["resultados"]["resumen"]["total_transacciones"] for a in user_analyses),
        "total_spent": sum(a["costo"] for a in user_analyses if a["pagado"]),
        "current_balance": user["balance"],
        "tier": user["tier"]
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
