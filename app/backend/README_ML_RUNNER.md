# ML Runner - Sistema de Inferencia Robusto

## ğŸ¯ Arquitectura

```
Usuario sube CSV â†’ Validador Enriquecedor â†’ pending/ â†’ ML Runner â†’ processed/
                                                                 â†“
                                                              failed/
```

## ğŸ“ Estructura de carpetas

```
app/backend/outputs/enriched/
â”œâ”€â”€ pending/           # Archivos enriquecidos esperando procesamiento
â”‚   â””â”€â”€ {analysis_id}.csv
â”œâ”€â”€ processed/         # Archivos procesados exitosamente
â”‚   â”œâ”€â”€ {analysis_id}.csv
â”‚   â””â”€â”€ {analysis_id}.json  # Resultados ML
â””â”€â”€ failed/            # Archivos con errores
    â”œâ”€â”€ {analysis_id}.csv
    â””â”€â”€ {analysis_id}_error.json
```

## ğŸ”§ Flujo de Inferencia

### 1. Upload (API)
```python
# enhanced_main_api.py - endpoint /api/portal/upload
1. Usuario sube archivo.csv
2. Se valida estructura bÃ¡sica
3. Se llama validador_enriquecedor con training_mode=False
4. Se genera outputs/enriched/pending/{analysis_id}.csv
```

### 2. Enriquecimiento (validador_enriquecedor.py)
```python
# Modo inferencia (training_mode=False):
- NO incluye columna clasificacion_lfpiorpi
- Genera 26 features numÃ©ricas + temporales
- Guarda en pending/ con nombre Ãºnico (analysis_id)
```

### 3. Procesamiento ML (ml_runner.py)
```python
# Lee pending/{analysis_id}.csv
1. Alinea features a PKL columns (one-hot + relleno de faltantes)
2. Ejecuta 3 capas ML:
   - Supervisado (Ensemble Stacking)
   - No Supervisado (Isolation Forest + KMeans)
   - Refuerzo (Q-Learning thresholds)
3. Aplica guardrails LFPIORPI
4. Guarda resultados en processed/{analysis_id}.json
5. Mueve CSV a processed/ o failed/
```

### 4. Retorno de Resultados (API)
```python
# Lee processed/{analysis_id}.json
- Cobra transacciones vÃ­a Supabase billing
- Retorna resultados al usuario
```

## ğŸš€ Uso

### Modo sincrÃ³nico (API Portal)
```bash
# El endpoint /api/portal/upload ejecuta automÃ¡ticamente:
POST /api/portal/upload
â†’ enriquece en pending/
â†’ ejecuta ml_runner.py {analysis_id}
â†’ retorna resultados
```

### Modo manual (debugging/testing)
```bash
# Procesar todos los pending
cd /workspaces/tarantulahawk/app/backend/api
python3 ml_runner.py

# Procesar archivo especÃ­fico
python3 ml_runner.py <analysis_id>
```

### Modo batch/cron (futura escala)
```bash
# Worker que procesa pending/ cada N segundos
while true; do
    python3 ml_runner.py
    sleep 10
done
```

## ğŸ“Š AlineaciÃ³n de Features

El runner usa `align_features()` para:
1. Eliminar columnas no usadas (cliente_id, fecha, clasificacion_lfpiorpi)
2. One-hot encode categÃ³ricas (tipo_operacion, sector_actividad, fraccion)
3. Rellenar features faltantes:
   - Dummies no vistas: 0
   - NumÃ©ricas: median del batch (o 0 si todo es NaN)
4. Sanitizar INF/NaN
5. Ordenar segÃºn `model_data['columns']`

## âš ï¸ Consideraciones para Escala

### Para 10k+ usuarios concurrentes:
1. **Storage**: Mover pending/processed/failed a S3/Supabase Storage
2. **Queue**: Usar tabla DB o Redis para encolar jobs (no filesystem)
3. **Workers**: MÃºltiples instancias del runner con pulling desde queue
4. **Idempotencia**: analysis_id Ãºnico + retry logic
5. **Async**: Endpoint retorna 202 Accepted + webhook/polling para resultados

### Actual (1 instancia, FS local):
- âœ… Maneja 1-10 usuarios simultÃ¡neos
- âœ… Atomic writes (tempfile + shutil.move)
- âœ… Nombres Ãºnicos (analysis_id = UUID)
- âš ï¸  Sin retry logic
- âš ï¸  Sin clustering (1 worker)

## ğŸ”’ Seguridad

- ValidaciÃ³n de tamaÃ±o de archivo (500MB max)
- SanitizaciÃ³n de features (INF/NaN)
- Timeout en runner (5 min)
- Error handling con traceback guardado
- No expone paths internos en errores pÃºblicos

## ğŸ“ˆ MÃ©tricas Clave

Monitor en logs:
- `â±ï¸  Tiempo de enriquecimiento`
- `â±ï¸  Tiempo de inferencia (3 capas)`
- `ğŸ“Š ClasificaciÃ³n: preocupante/inusual/relevante/limpio`
- `âš–ï¸  Guardrails aplicados`
- `âœ…/âŒ Exitosos vs fallidos`

## ğŸ› Debugging

```bash
# Ver archivos pendientes
ls -lh /workspaces/tarantulahawk/app/backend/outputs/enriched/pending/

# Ver resultados procesados
ls -lh /workspaces/tarantulahawk/app/backend/outputs/enriched/processed/

# Ver errores
cat /workspaces/tarantulahawk/app/backend/outputs/enriched/failed/*_error.json

# Logs del runner (si se ejecuta manualmente)
python3 ml_runner.py 2>&1 | tee runner.log
```

## âœ… Smoke Test

```bash
# 1. Preparar CSV de prueba
cat > test_input.csv << EOF
cliente_id,monto,fecha,tipo_operacion,sector_actividad
12345,50000,2025-01-15,efectivo,casa_cambio
12346,180000,2025-01-16,transferencia_nacional,inmobiliaria
EOF

# 2. Enriquecer (modo inferencia)
cd /workspaces/tarantulahawk/app/backend/api/utils
python3 validador_enriquecedor.py test_input.csv random null false test_001

# 3. Verificar pending
ls -lh ../../outputs/enriched/pending/test_001.csv

# 4. Ejecutar runner
cd /workspaces/tarantulahawk/app/backend/api
python3 ml_runner.py test_001

# 5. Verificar resultados
cat ../../outputs/enriched/processed/test_001.json
```

## ğŸ“ Notas de ImplementaciÃ³n

- **PKL columns**: El runner lee `model_data['columns']` de cada PKL
- **No config editing**: No se edita config_modelos.json por upload
- **Atomic moves**: Evita condiciones de carrera en FS
- **training_mode**: SeparaciÃ³n clara entre entrenamiento e inferencia
- **Billing**: Solo se cobra despuÃ©s de ML exitoso
